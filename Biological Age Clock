        if ensemble_train:
            learning_rates = [.01]  # , .001]
            min_lrs = [.002]
            patiences = [8]
            layer_sizes = [93]  # , 125]
            num_layers2 = [5]
            noises = [.08]  # [.2, .1, .01, .001]
            l1s = [.034]  # [.2, .1]
            l2s = [.085]  # [.2, .1]
            b1s = [.99]  # [.5, .9, .99]
            b2s = [.999]  # [.5, .9, .99]
            dropout_rates = [.02]
        else:   # Current best: ChronoAge0.01-0.0019247923925306844-8.0-93.0-5.0-0.05635867926717027-0.034533334567416266-0.08505781598392871-0.011274876020738067
            learning_rates = [.01]  # , .001]
            min_lrs = [.001, .0005]
            patiences = [14]
            layer_sizes = [125]  # , 125]
            num_layers2 = [7]  # 7 current best
            noises = [.4]  # .4 current best
            l1s = [.005]  # .005 current best
            l2s = [0.001]  # .01 current best
            b1s = [.99]  # [.5, .9, .99]
            b2s = [.999]  # [.5, .9, .99]
            dropout_rates = [.3]   # .3 current best
        # df_log = pd.read_csv('data/performances/GridSearchPerformance.csv').set_index('Unnamed: 0')
        df_log = pd.DataFrame(
            columns=['Learning Rate', 'Min Learning Rate', 'Patience', 'Layer Size', 'Number of Layers', 'Noise', 'L1', 'L2',
                     'Dropout Rate', 'Val_RMSE', 'Val_MAE'])

        if load_model:
            model_name = 'ChronoAge0.005-125-9-0.01-0.07-0.1'
            model = keras.models.load_model('models/' + model_name)
        else:
            if ensemble_train and ensemble_train or not ensemble:
                if auto_tune:
                    space = {'min_lr': hp.uniform('min_lr', .0001, .05),
                             'patience': hp.quniform('patience', 8, 15, 1),
                             'layer_size': hp.quniform('layer_size', 80, 150, 1),
                             'num_layers': hp.quniform('num_layers', 4, 8, 1),
                             'noise': hp.uniform('noise', .01, .4),
                             'l1': hp.uniform('l1', 0, .09),
                             'l2': hp.uniform('l2', 0, .09),
                             'dropout_rate': hp.uniform('dropout_rate', .01, .4)}
                    hyper_algorithm = tpe.suggest

                    def tuning_objective(hyperparameters={}):
                        min_lr, patience, layer_size, num_layers, noise, l1, l2, dropout_rate = hyperparameters['min_lr'], \
                                                                                                hyperparameters['patience'],\
                                                                                                hyperparameters['layer_size'],\
                                                                                                hyperparameters['num_layers'],\
                                                                                                hyperparameters['noise'], \
                                                                                                hyperparameters['l1'], \
                                                                                                hyperparameters['l2'], \
                                                                                                hyperparameters['dropout_rate']
                        model_name = 'ChronoAge' + str(.01) + '-' + str(min_lr) \
                                     + '-' + str(patience) + '-' + str(layer_size) + '-' \
                                     + str(num_layers) + '-' + str(noise) + '-' + str(l1) \
                                     + '-' + str(l2) + '-' + str(dropout_rate)
                        if chaining:
                            model_name = model_name + ' Chain Model'
                        if ensemble_train:
                            model_name = model_name + ' Ensemble'
                            file_path = 'chunk models/' + model_name + c_name
                        else:
                            file_path = 'models/' + model_name
                        print(model_name)

                        callbacks = [keras.callbacks.EarlyStopping(monitor='loss',
                                                                   patience=patience,
                                                                   mode='min',
                                                                   restore_best_weights=True),
                                     keras.callbacks.ReduceLROnPlateau(monitor='loss',
                                                                       patience=int(patience / 2),
                                                                       factor=.5,
                                                                       verbose=1,
                                                                       mode='min',
                                                                       min_lr=min_lr), ]

                        opt = keras.optimizers.Adam(learning_rate=.01, beta_1=.99,
                                                    beta_2=.999)

                        mod = keras.Sequential(layers.Dense(len(train_x[0]), activation='relu'))

                        for layer in range(int(num_layers)):
                            mod.add(keras.layers.BatchNormalization())
                            mod.add(keras.layers.GaussianNoise(noise))
                            mod.add(keras.layers.ActivityRegularization(l1=l1, l2=l2))
                            mod.add(keras.layers.Dense(layer_size, activation='relu'))
                            mod.add(keras.layers.Dropout(dropout_rate))

                        output_layer = mod.add(keras.layers.Dense(1))  # , bias_initializer=initial_bias))

                        mod.compile(optimizer=opt,
                                      loss='mean_squared_error',
                                      metrics=metrics)

                        mod.fit(x=train_x,
                                  y=train_y,
                                  validation_data=[test_x, test_y],
                                  batch_size=batch_size,
                                  epochs=epochs,
                                  callbacks=callbacks,
                                  verbose=1)

                        mod.save(file_path)
                        print('Save path of model: ', file_path)

                        # loss = mod.evaluate(dasat_x, dasat_y)
                        loss = mod.evaluate(test_x, test_y)
                        # loss = mod.evaluate(validation_x, validation_y)

                        print('Parameters: ', min_lr, patience, layer_size, num_layers, noise, l1, l2, dropout_rate)
                        print('Loss: ', loss, '\n')

                        return {'Loss': loss, 'Params': hyperparameters, 'Status': STATUS_OK}

                    best = fmin(fn=tuning_objective, space=space, algo=tpe.suggest,
                                max_evals=200, trials=Trials())
